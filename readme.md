# Azure Design Technologies for Cloud Configuration

I have created this repository for my self learning and reference for expert Cloud Architect certifications

## Deployment cycle for application

- **Create a VM and a virtual network**
    - Specify inbound ports - The RDP is usually automatically selected, you can also add port 80
    - You will get an OS level disk as well as a temporary D drive, you can also add an additional disk for database
    - Select default encryption at rest
    - The VM can have a public IP address if it is hosting a web application that will be accessed by the public
        - A public IP address can also be added at a later time
    - Create a **virtual network** as you creat a VM 
        - Primary virtual network interface - has its own IP address, used for internal communication
    - An NSG is also created the controls permissions for the ports

- **Create a Storage account**
    - Azure blob service - The storage capacity automatically grows
    - While you can store data on your local VM - the storage account offers the flexibility of increasing capacity over time, limited options with the VM
    - Isolation of compute and the storage environment - data or the application are not dependent on eachother - especialy if the VM crashes, you can always create a new VM and connect it to the same storage account
    - Specifications
        - Account name, Geo-redundant storage, locally redundant storage is the least expensive
    - Go to the `Storage account resource || Under data storage: Containers (blob service), fileshares (across VMs), Queues (messaging), tables (No SQL like data)`

- **Creating an Azure SQL database** Platform as a service (PaaS)
    - Note that the SQL server, Database and the storage account must be in the same region
    - When you use AZURE SQL, you do not have to manually manage the underlying VM hosting the database, which means that you do not have to install the Azure SQL service engine, no security or patching of the database
    - You can create the SQL database from the main resources as usual
        - Specifications
            - `Create a server `- This is for logging into the SQL database server. It has 2 components, the server that hosts the database and the databse itself
                - Server name- must be unique
                - Admin login details
                - Select pricing models for database - `basic,standard, premium` - These are based on DTUs, data transfer units - costs are estimated per month
                - Select backup storage redundancy
                - Specify the connectivity options - a firewall is connected by default - options: No access, public endpoint, private endpoint
                - You can add your current computer IP address to the firewall rules as well as permit Azure service/resources to access the server
                - You can use **SQL server management studio** as you work with SQL databases - install this tool on your local machine to access the server 
    - Once created , you can locate the database in resources by its name, you can run the SQL server management studio
        - The server name is the webaddress of the SQL server which you can locate from the database resource homepage that you created and you use the login credentials in order to connect

    - **Building an appliation**
        - Will be hosted on an Azure VM and Azure webapp
        - The first part is setting up MYSQL server and a storage account and creating the required tables in the MYSQL database
            - **Setting up tables in the SQL database**
                - Use SQL server management studio to create tables and insert values
                - Select the database by name right click - new query and then you can paste your commands > click execute
                - Note that in the database, only the image links are stored, but you can store the images in the Azure storage account
            - **Uploading the images into the storage account**
                - Go to the `storage account || containers > create a new container and call it images > select Blob under public access level`
                - Open images and upload the file. You will get the image url
        - **Writing the code for application**
            - You can use various backend languages like .net, node, java. This program will have a frontend and a backend
            - The backend portion will contain the queries for database
        - **Setting up a webserver on VM to publish application**
            - Go to the `server dashboard > roles and features> select role based> check Webserver IIS > next and then check management services under management tools to allow solutions from virtual studio`
            - Open internet information services manager
                - Select management service and then click enable remote connections. Then click apply and start - this ensures that the webserver will take requests from virtual studio
                - Turn off enhanced security configuration from the main dashboard
            - Then you can download and install the .Net core 3.1 to configure the server to run a .net application - you have to select the `windows hosting bundle` - There is a different installation file based on the type of OS - for example there is a different one for linux
            - Next you have to install another software `web deploy` which deploys the application on to the internet information services
                - **Configure a DNS name**
                    - Go to the VM resource > click not configured under DNS name > type a DNS name label
                    - Go to the VM resource || networking > Add inbound rule to allow port 8172, the port that you assigned with the management service when configuring IIS on the VM
        - Once the installations are complete. You then have to create a `webapp resource` in Azure
            - **Create webapp from main resources > Create a resorce**
                - Give it a name
                - Select code vs container for now and select runtime stack as .Net core 3.1(LTS)
                - Select a region, and it will automatically select a new `app service plan`, you can also `select the sku such as the basic app service plan`
                - For the monitoring of your app you can create a new resource called `application insights` - give name and specify the region
                - Once the deployment is complete, go to the resource, you will find the url of the webapp. If you paste it in the browser, it will show a default webpage
                - Once all these steps are complete, the app will be deployed on the Vm as a webapp

        - **Application Monitoring**
            - Finally after the app has been configured and deployed. You can then setup monitoring, which will assess logs and you can configure alerts etc
            - You can select between static and dynamic routes
            - You can also create action groups, dictating what to do when a threshold is met. You can configure to e-mail and administrator or you can run scripts in response to an alert
        - **Configure logs** 
            - **Create Log analytics workspace** and connect the VM to it
            - Install an analytics agent on the VM this is what 'connect' does
            - You can then configure the agent to specify what type of logs are you looking to transfer to the workspace like events performance etc
            - Logs can be grouped into various categories

        - **NSG flow logs**
            - Captures logs for traffic - IP flows - useful for the security team
            - Useful information like source and destination IP and ports and the outcome of the address
            - You can locate the NSG group by goting to the `VM resource || networking`
            - When you click on the network security group, you will see there is an option for `flow logs` - turn it on, select the version- version 2 has additional information about the packaging of bit size etc
            - Logs are stored in the storage account - so a storage account must be linked with flow log settings. Once the account has been linked, you can verify it by going to the storage account and you will notice a new flow logs container
                - **Network watcher**
                    - This is another service that analyzes the traffic
                    - Search for network watcher from general resources `Network watcher || NSG flow logs > turn on traffic analytics status`
                    - Then also select the log analytics workspace
                    - You can specify the processing interval to create visualizations
                    - To view these visualizations go to `Network watcher || traffic analytics`
                    - There are options for malacious traffic and blocked traffic for a more granular analysis
                    - **Service Map**
                        - This feature offers a more in depth assessment of what processess are running on the VMs - Also provides the external ips and port numbers the VMs are connected to - in the form of a 'service map'
                        - Go to `workspace resource || Service map` to issue an agent. This can be done with a command that will install an Azure monitoring dependency agent as an extension
                        - Note that when you are first running the powershell, you may have to select a storage account for filesharing before you can run the commands
                        - Below is the command to install this agent
                        - `Set-AzVMExtension -ExtensionName "Microsoft.Azure.Monitoring.DependencyAgent" -ResourceGroupName "new-grp" -VMName "demovm" -Publisher "Microsoft.Azure.Monitoring.DependencyAgent" -ExtensionType "DependencyAgentWindows" -TypeHandlerVersion 9.5 -Location NorthEurope`
                        - The service map now created, needs to be added to the workspace
                            - `workspace resource || workspace summary under general > add > in th search type service map > Create service map solution`
                            - Once created, under the `workspace resource || summary` you will be able to see the service map
            
            - **Azure SQL diagnostics**
                - GO to the `SQL DB resource || disgnostics under monitoring > Add diagnostic setting`
                - Then you can stream various types of logs like errors, timeouts, basic metrics, by selecting their category and then also selecting the destination details by checking 'send to log analytics workspace' Then you can select the log analytics workspace reseource
                - This setting will forward all the SQL diagnostics data to log analytics workspace

- You can review all the sign ins to the Azure AD by selecting the Azure Active directory from the main left panel and then you can select Users
- You can click the `default directory` from top left and then select `sign-ins` under monitoring - It will show all the sign in activity
- You can add a diagnostic setting - where you can select various type of logs such as `audit logs, signin logs` etc to a certain destination including analytics workspace or storage account or an event hub
    - Once this setting has been enabled. You can verify it by going to `work space resource || log management`

## Costing Estimates
 - Pricing calculator - you can get to it by searching for `Azure pricing calculator` 
    - There are various categories for which you can estimate costs like VM, storage accounts, SQL etc
    - **VM**
        - Windows VM cost more than linux due to licensing, almost double the price/mo
        - Region also effects price
        - Disk size and the duration 
        - Compute and Disks costs are separate. The compute costs are the running costs - even if the VM is shut off, you will still be charged for disks
        - Storage transactions also cost
        - Transfer of data on to different region can be costly
    - **Storage account**
        - Capacity
        - Write operations
        - Container and read operations
- Likewise there are other components which you can look into such as database, Kubernetes, Azure functios
- You can also look at cost-analysis by subscription
- You can select you `subsciption || cost analysis` which will give you break down according to service name, location, resource group
- Optimizing costs
    - It the resourse is not being used, then delete it
    - You can look at it by going to the resource homepage and under the `monitoring` tab you can review whether a particular resource is being used
    - If the CPU usage is less than 6% then it can tell you that you can possibly reduce the instance size 
    - This way you can determine what the sizing of the resource can be, the catch is that there is always some downtown with changing the size - so this should be tested in the staging environment rather than production
- **Azure advisor service** from the main dashboard 
    - It can provide recommendations based on cost
    - For example it can inform about a reserve instance to save on cost over a VM that is running for a long time, but not being utilized
- **Budgets**
    - You can go to your `subscription || budgets under cost management` You can define a monthly amount, if that is exceeded, you will get an email notification
- **Azure Reserve Pricing**
    - It is a huge saving opportunity - if you buy a 1-3 year commitment to reserve instance
    - This is bascally a discount that is offered with a commitment to use that reserve for a specific duration of period
    - You do have the ability to cancel a reservation

- **Reviewing subscriptions based on tags**
    - Tags can be used to mark certain departments within a company
    - You can then filter costs for a particular department based on tags
    - Select a particular `subscription || cost analysis under cost management > under top bars with filter`, select the tag for the department you are looking to analyze costs
    - Tags work better at the level of an individual reseource rather then at the level of a resource group

- **Application insights**
    - Allows integration of application insights service with Azure webapp service can let you track live stats about the website
    - This offers infomation about how the users interact with the application e-g page views
    - If you work on a project through visual studio, there is an option to work with `application insights telemetry`
    - You will be able to see th GET requests, response code and response time
    - Visual studio can send the telemetry application insights data to a resource on Azure
    - On **Azure portal**
        - Create a webap resource from general resources
        - While creating the application insights resource, under monitoring you can add **Application insights** resource
            - you can then search the `application insights resource from resources || live metrics`
            - Before you will see anything on the live metrics you will have to publish your project from visual studio into the webap
            - You can also review performace of the appliation by going to the `application insights resource || performance`. Likewise other usefule features include `failures` and `users`
            - You can also configure `smart alerts `that utilize machine learning 
                - You can go to the `application insights resource || smart detection > settings - you can find smart detection rules`
            - You can also use the `continues expert feature` for data analysis
                - You can go to the `application insights resource || continues resource > add` Where you can specify what type of data you want to send to your storage account
 - **Azure Sentinel Service**
    - This is an Azure Cybersecurity Instrument that can perform SEIM (Security Information Event Management) and SOAR (Security Orchestration Automatied Response)
    - It consists of `connectors` that connects the data to services. For example Azure sentinel service can be connected to the Azure analytics workspace via the connectors
    - Data connectors also allow you to connect to the external services
    - Cybersecurity cycle: Visibility, Analytics, hunting, Incidents, Automation
    - **Azure Security Center** allows you to look at the visibiity and analytics phase of the cybersecurity cycle
    - Azure sentinel allows you to look at all the phases of the cybersecurity cycle - It is a separate resource than Azure sentinel
    - Then there are builtin workbooks and queries
    - You cannot enable the log analytic workspace to Azure Sentinel if you have already linked it with Azure Security center
    - `Steps`
        - Create a log analytics workspace
            - Select resource, location, name (e-g `sentinelworkspace`)
        - Select Azure Sentinel from the main resources || data connectors
            - Among the connectors you can select `Azure Active Directory` Then open up that connector 
            - There you can check mark the type of logs that can be sent to Azure Sentinel from the Azure Active Directory - these can incude sign in logs, audit logs etc
            - Likewise there are multiple connectors that you can link to Azure Sentinel, such as you can add `Azure activity` This will send all of Azure activity logs from Azure subscription to Azure Sentinel
            - Another useful connector is `Security events` from windows VM machines. This will require installation of an agent onto the windows VM
                - If you want to add Security events then you will also have to add that particular VM to Sentinel workspace
                - `Sentinel workspace || virtual machines > Select the VM from list > click connect` The connect here will install an agent on the VM that will manage transfering event logs to the sentinel workspace
                - You can manually look at the security events on a windows Vm by RDP to the VM and then opening `event viewer` > windows log > security there you can see all the recorded security events
        - **Workbooks**
            - If you go to `Azure sentinel resource || data connectors`, Once you select a data connector then under the `next` tab you will see a set of recommended workbooks > view templates - you can visualize a lot of the data that way
            - The benifit of looking at these workbooks is that you do not have to look at the `event viewer` by manually going to a machine

# Azure Security and Identity

- Two main steps: Authentication - verifying your username and password, the next step is authorization for a resource - this occurs at the subscription level and is user specific. 
- Hierarchy -> Resources > Resource groups > Subscriptions > Management groups (group subscriptions together) > Tenant root group
- Note that the permissions get inherited as you move down the hierarchy

## Azure Active directory

- You can create users, groups, roles and administrators
- Azure AD connect lets you connect your on premises users with the Azure active directory
- You can look at all the features available with the Azure active directory onto the Azure active directory pricing webpage
- The premium version of active directory also offers
    - Self-service password reset
    - Conditional access policies
    - Identity governanace, access reviews and privilidged idendity
    - The pricing is per user per month
- **Trust relation b/w Azure active directory and subscription**
- The subscription must trust the user in the Azure active directory, before it will allow access to the resources
- Go to Azure subscriptions > change directory - by default it will select your default directory - however you can creat addtional directories and then you can tell the subscription to trust the new directory
- There are pre-built roles that you can assign. 
    - 4 high level roles: Contributor (Cannot allow permissions for additional roles), owner, reader, User access Administrator
- **Creating a new user in Azure Active Directory**
    - `Active directory || users > new user` Its basically like creating a new e-mail based on the website that is linked to your Azure account
        - While you create the user you give it a username and password
        - The first time log in will require the user to change the password
        - Then you can sign in with this new username from the same Azure portal
            - This user will not be able to see any of the resources
    - You can grant acces to a user to a particular VM
        - Go to the `VM resource || Access control > Add > Add role assignment > reader role > Under select search a user`
        - Once this access has been granted, if you log in with the same user account. You will now be able to see the VM on the dashboard
        - This user will only be able to have limied access to this VM as we previously only permitted `read` level access - for example the user will not be able to see the IP address and subnet associated with the VM - these are separate resources and the user did not have access to these
        - To overcome this you can grant access to the user at the resource group level in addition to the VM level to which the VM is part of
            - Once this is scuccessfull, then when you log in with this user you will now be able to see all the resources listed on the dashboard if you log on to the resource with this user
            - This user will not be able to stop the VM as this user was not granted with `contributor` level access
- **Azure Active Directory roles that can be assigned to the users**
- Note that the Subscription based roles (Role based access control - subscription level) is different from Azure Active directory roles
- Role based access control is more of an administrative level permission as to what level of permissions this user can further assign, where as Azure active directory roles are limited to the resource group access
        - **Dedicating some administrative roles to the users**
            - While logged in as the root admin Azure active directory || Roles and administrators
                - There you can see what role is currently assigned to the account - for the root admin its `global administrator`
                - Then you can select a particular user from the search and assign it a role
                - From the default root user account `Azure AD || Users > find the user > Assigned roles > Add assignment > Select role of User administrator` - This will allow the user to manage the users - including creating new users
                - You can also decide the assignment type elligible vs active (immediate)
- **Azure AD privilidged Identity management** - This feature is only available in Azure premium P2 license
    - The global administrator assigns special permissions to one of the user (prvilidged user) that can intern perform global administrator like functions such as creating or deleting VM ir SQL databases
    - The problem is that If this user moves onto a different department, this privilidged access will need to be removed
    - To avoid this problem, Azure AD privilileged identity management can be used - the benifit it offers is of temporary nature
        - **Features**:Just in time and time-bound and requiring activation of privilidged roles, implement multi-factor authentication to active role and setting notifications when a privilidged role is activated, conduct access reviews
        - this feature can only be implemented with `premium access` of AD
    - Select the `privilidged identity management from general resources` 
        - There are two pathways, one is the active pathway the other is the eligibility pathway
        - In the active pathway, the privilidges go in effect right away - when setting this up you have to options of 
            - `Privilidged identity management || AZure AD roles under manage - THis leads to the default directory homepage || settings > Search for the user administrator role > edit`
                - On this page you will see the tabs of Activation, Assignment and Notification
                    - **Activation** - you can select where multi-factor identification is to be applied
                    - You can select max activation duration - this is the number of hrs the user with this privilidge have to perform certain tasks once activated
                    - **Assignment** - Here you can specify whether you want it to be a permanent elligible assignement or a permanent active assignment OR you can specify an expiration time (e-g expire after 15 days). The elligible option offers an additional layer of security as it does not go into effect right away, the user who is rendered this elligibility will have to perform the activation from his/her account
                    - **Notification** - Who should be notified when the role has been assigned
            - `Azure AD || users > search the user > click the user || assigned roles` - here you will be able to see the assignment
            - **Adding a new assignment**
                - `Azure AD || users > search the user > click the user || assigned roles > Add assignment > select user administrator role`
                    - Select assignment type as eligible - It will have a start and end date - This date range is only when this particular user will be eligible to take up this role
                    - Next if you go inside this user's account by logging in you will see that this user will now be able to activate this elligibility
                        - Home page after logging in, Azure active directory || all users - wont be able to add users yet
                        - search `privilidged identity management || my roles > there you can see the eligibility to take up the user administrator role`
                    - The elligibility will be available only during the specified dates, during which the user can activate this additional privilidge and once the privilidge is activated there will only be a set number of hrs during which the user can perform the task. However, within the eligibility period. The user will be able to reactivate for this role multiple times
- **Azure Reviews**
- Review to access whether the latest user roles are appropriate for group memberships and role assignments
- Reviews can be conducted for:
    - Security group members
    - Users assigned to an application
    - Azure AD role
    - Azure resource role
- For reviews in the AD and resource roles, the reviews must be conducted with the privilidged identity management
- Premium tasks
    - Member and guest users who are assigned as reviewers
    - Member and guest users who perform a self-review
    - Group owners who perform an access review
    - Application owners who perform an acces review
- P2 license is not required for uses with Global Administrator or User Administrator roles
- Go to the `default directory, privilidged identity management || access reviews > new`
    - There you can decide the date range and the frequency of reviews
    - Then you can select the role for which you are performing a review e-g review of all the user accounts that have `user administrator` privilidges
    - you can also select `reviewers` these are the user accounts that can conduct reviews - If you are doign the review yourself then you can add your user account
    - It will display a list of users for each of the user, you can click audit to see the logs
    - Here you will have to opportuity to `approve or deny` which will continue or take away the privilidge, e-g in this case it is user Administrator
    - Then if you go back on to `default directoy || access review > you will see an access review - click it to modify it`. 
        - If you have dinied the user then you will first have to `stop` the access review and then click `apply`

- **Discover Resources**
- `Privilidged identy management || Azure resources`
    - It will show the subscriptions e-g Test environment
    - Going to the subscription homepage also gives you a summary of Identity management activities
    - Here you will see various roles for example a basic reader role - this will let you modify it
        - You will have to options of setting up activation, assignment and notification settings

- **Azure AD identity protection**
- It is to detect problems logins
- It will analyze the day to day logins of a particular user and then determine any risk to the user as well as the organization
    - For example the login is coming form an ip address that is not really known
    - It can categorize the risk as high medium or low
    - Then you can also trigger an action for example block access, but trigger a password change
    - Utilization of these features like user risk policy and sign in risk policy require Azure AD premium subscription

- **Identity governance**
    - Search for a`ccess review in general resources || access reviews > New access review`
        - Here you can select teams and `groups` and select `scope` like all users etc
        - Next in the `reviews` tab, then you can select reviewer, select the admin account and select the frequency with which to perform the review. Then likewise you can select the days and the start date - you can also specify frequency like weekly review, monthly review etc
        - Asign it a name and then click create
        - You will get an email notification when the reciew is complete - through which you can take actions like approve or deny an access
        - You can see information about a user like their last day of signin - there will be automatic recommendations and if you agree with them you can accept of deny the recommendations or approve or deny separately

- **Azure Security Center**
    - You can go to this resource 
    - Gives an overview of the security posture of the resources
    - It provides an overall security  score
    - It scans the resources and gives you recommendations
    - Some recommendations are free, but for some more advanced recommendations you will have to pay
         - `Security center || overview || pricing` and settings will enable standard tier
         - Navigating to this tier you can see whats available for free and whats available with costs e-g with standard tier
         - E-g regulatory compliance dashboard and reports will not be available for free
         - Continues assessment and security recommendations as well as Azure secure score are fee features
         - If you scroll down further, you will see a list of all of your resources and you can select whether to enable a recommendation - This is based on `Azure policies`
         - **Azure Defencer**
         - `Security ceter || Azure defender` is another layer of security - It can be turned on under pricing and enabled for specific resources
    - **Just in time VM access feature of Azure defender**
        - `Security Center || Azure defender > scrolling down you will be able to see just in time VM access`
            - You will be able to see the unprotected VMs
            - Click the `just in time > under the not configure tab`
                - Then select the VM and click `enable JIT` This will lead you to configure ports - you can delete the ports that you do not need them to be open. Then click `save`
                - Once you have implemented this, you can double check it by going to that particular VM resource and under networking you can see the rules
                - You can now see in the VM resource || networking that a new rule has been created with a new priority and a default setting of `deny`
                - If you want your computer IP to obtain access, you can then either go to the `VM resource || connect > request access`
                - Alternatively you can go to `Security center || Azure defender > Just in time VM > request access`
                    - Here you can then select `myip` or enter an ip range and click `open port`
                    - This will add another security rule permiting the specific ip address


- `Azure Identity protection` - Detect suspiscious signins - more user specific
- `Azure privilidged identity protection` - Azure AD roles and Azure resources - more about granting privilidged access to a user
- `Azure Securty center` - lets you configure just-in-time VM access

- **Multi-factor authentication**
    - You can enable multi-factor authentication per user
    - `Azure active directry || all users > from top left drop-down select Multi-factor authentication`
    - You will see a list of users and then you can select and then click `enable`
    - Then the user when he/she logs on will be prompted to fill in infomation regarding which option to use for authentication such as a phone or an app

- **Conditional access policies** requires Azure AD premium license
    - `Default directory from dashboard || security || conditional access > new policy`
        - you can select the users or usergroups to which the policy should be applied
        - Next you can select the apps to which you want to policy to be applied to - for the azure portal its `microsoft azure management`
        - Under `access control > grant` There you can select whether you want to require multi-factor authentication for this group
        - You also have the option of at least one or all of the controls to be satisfied
        - Then `enable policy > on`

- **Azure Resource Locks**
    - This feature is designed to prevent accidental modification and deletion of critial resources
    - Locks can be applied at the resource level, subscripion level or resource group level
        -`Cannot delete lock` - Can update but not delete
        - `Read only lock` - Can neither update nor delete
    - Go to the `resource || locks > add` there you can also see the options for resource group and subscription
        - For example if you apply this option to a virtual machine resource, you will have the lock-type property where you can specify the type of lock you are looking to apply. For example if you chose the read only lock, you will not be able to increase the size of a VM
        - Even the global admin cannot override the lock
        - If you apply the lock at a resource group level, then the lock is inherited by all the resources within the rerource group

- **Azure policies**
    - There are general policies for a coorporation to manage restriction on how the architects can deploy resources, e-g a general policy setting a limit on the size of VMs that can be deployed
    - The will prevent from creating new resources that surpass the policy restrictions, but for the resources that are already created it will only mark and list them for you to take action
    - Search for `policy` in the general resources - This will lead to a policy dashboard that displays policy assignments
        - From the `policy dashboard > under categories > uncheck all and select compute` This will display all the policies in this categories that are available from Azure to be applied to resources
            - There if you select `allowed VM size SKUs` and then click `assign` - here you will have the option to select a subscription or a resource group
            - You also have the option to add exclusions
            - Here you can select the VM sizes that are allowed
    - Once the plolicy has been applied to a subscription, you will then be able to see the resources listed that are not compliant with that policy

- **Azure Blueprints**
    - Consists of `Artifacts` : - ARM templates - To create infrastructure
                                - Azure policies - To govern creation of infrastructure
                                - Resource groups
                                - Role-based access control - Creating users, or automatically assigning roles to a given user
    - These are the default architecture settings of a company to govern the deployment of resources
    - These are usually helpful for organizations with multiple management groups with multiple subscriptions
    - You create a blue print and then assign it to a subscription or a management group. If it is saved to a management group
    - Steps: `Defining, publishing and assigning`
    - Only a user with `contributor` level access can save a blue print
    - You can protect a blue print by applying resource locks
    - There are some pre-existing samples in Azure that can be used when designing blue prints
    - Even if you are the `tenant-root group` - the Root level user of the Azure account, you still need role assignment before you can create blue prints
        - In other words you will have to assign `owner` role or `contributer` role to the root account - The point is that even if you are the King administerator, this assignment is not in place by default and will need to be configured before this account can be used to create blue prints
    - You can also **Unassign** a blue print by going to `Blueprints || Assigned blueprints > right clicking the blue print and selecting unassign`
        - Note that unassigning a blue print does not undo the resources created when this particular blue print was published
    - **Creating a blue print**
        - `Searh blueprint in general resources || blueprint definitions > create blueprint`
            - You can either start with a sample or you can click `start with black blueprint`
                - **Basic tab**
                - Give it a name, select **definition loaction** (choose subscription or management group - even if you have no specific management groups, you will have a default tenant group, not that you must have at least `contributor` level access to be able to assign a subscription otherwise you will get an error)
                    - You can assign contributor role by goiing to `tenant root group || Access control > assign owner role` Note that this step has to be implemented and is not always there by default - so be sure to check that before creating blue prints - you will have to wait for 5 min before the role gets assigned
                - **Artifacts tab**
                    - Click `add artifact` - then in the pop up you can select artifact type: `policy, role, resource group, resource manager`
                    - Choose resource group - the name given here is the name for the artifact, not the resource. The resource group name is created dynamically - this will extract the resource name as a parameter from an existing resource or resource group to which this blue print will be assigned
                    - You can also assign static values for example a blue print maybe assigned only to a particular location
                    - You also have the option of assigining tag names
                    - Likewise you can add another `artifact as a policy`, so that which this blue print is published, a particular policy is automatically assigned. For example a policy for automatic backup of VMs
                    - Likewise you can add another `artifact as resource manager` - Here you can add JSON code to configure infrastructre, such as creating a storage account
                    - Likewise you can add another `artifact as role assignment` - Here you can select a particular user and give it an automatic role assignment such as `owner` whenever this blueprint is applied
                    - Once congifuration is complete and the artifacts have been added to the blue print, click `save draft` After this step, the blue print will be created in an `unpublished` state - which means that it has yet to be assigned
                    - You can go to `Blurprints || blueprint definitions` there you will see the list of blue prints that are created

                - **Publising and assigining the blue print** - This step deploys the blue print to create resources
                        - Select a blue print and right click ` publish blueprint`  specify the version  and click `publish`
                        - After the blue print has been published in `blueprints || blueprint definitions` select the blue print from a list that you just `published > right click > select assign blueprint`
                            - Here you will slect a `subscription`
                            - Select lock assignment such as do not delete or readonly option for the resources created
                            - Managed identity select `system assigned` this gives Azure temporary `owner` role in order to deploy all artifacts
                            - You also have the option of creating your own user managed identity if you select `user assigned` instead
                            - Next you will add in information about the resource group name - that you are now dynamically providing while deploying this artifact, you will also se the static assignement such as the location that you specified while creating the blue print
                            - Likewise you can fill in all the options for the dynamic resources that are to be created as part of this blue print, like how many VMs should be created, how many storage accounts should be created etc
                            - Finally click `Assign`
                            - Note that with the assignment of the blue print to the subscription, it will run the blueprint and creat all architechture policies etc in that subscription
                            - Then you can verify successful deployment by going to all resources and select your subscription
                            - You can also see the new resource created by its name under all resources. If you go to this r`esource group || access control > role assignments` you can see the users and their level of role assignment e-g where its limited to this particular role or the management group
                            - Likewise you can go to `policies` you will see a new policy that has been setup

    - **Applying Resource locks to a blue print**
        - `Blueprints || blue print definitions` you will see the exisiting blueprints > right click and select `assign blueprint > choose perscription,blue print version, select **do not delete**`
        - This action will automatically apply the feature of `not being able to delete` to all the resources that are created as part of this blue print
            - Even if you go to an individual resourse such as a storage account and go under `locks` you will not see anyindividualized locks, if this resource was created as part of Azure blue print which contained a policy artifact of not being able to delete any resources created when this blue print was published, the storage account will inherit this trait, and you not be able to delete it - even if you are the owner
                - If you want to delete it you will have to got to `Blueprints || assigned blurprints`and then `unassign` it from the subscription

    - **Application Objects**
        - For example application wanting to access to a storage account
        - It is like giving an application its own automatic user to access the storage account. The application object is the user for the application
        - `Azure active directory or default directory || app registerations > application object`
            - On this homepage you obtain the client ID, object ID and Tenant ID - these are specify markers of access that can be implanted onto the app object to match before connecting to the app object
            - In addition you can also create a client secret that you can store vault
            - You can create a secret by going to the `app resource || certificates and secrets`. You can copy the value of the secret and paste it into the app
            - If the app is trying to access an existing storage account, you will have to paste the URl of the storage account into the app
            - You can further configure authentication by going to `app resource || authentication`

    - **Azure Key vault**
        - Storing enctyption keys, certificates and secrets
        - Key vailt is the security guard between the application and database that checks application IDs
        - You can create a key vault by searching key vault in the resources and clicking create
            - Select a resource group, give name, location, slet the days for retaininng vault after deletion
            - Slect vault access policy
        - To create a secret you go to `key vault resource || secrets > generate`
        - From the key vault you can get the vault URI which you will have to link with the app
        - The connection link between the app and the Azure key vault is the secret and the connection between the Azure and App is a policy inside the vault that recognizes the application
            -`key vaule resource || access policies > create a new policy`
    
    - **Manged identities**
        - This is an alternative to application object
        - Here you give an identity to an Azure resource (for example a VM) as if it were a user that can directly access the storage account
        - To the identity you grant role based access control
        - Here you do not have to embed any details like client id, tenant id or secret into the application
        - You have to first enable the managed identity on a particular azure resource like a VM for example
        - Pre-requsits: Create a VM resource and a storage account resource. Inside th storage account create a container and in the container upload a text file
            - Go to the `resource || identity > Turn status on > save` - This step registers the resource with Azure active directory
        - Next you can go to the `storage account resource || access control >add`
            - This will open up add role assignment window
                - choose the role, e-g `reader`
                - under select select the rsource whose 'manage identity' you have created, VM in this case for example. If you have already created a managed identity for the VM then that VM will show up here automatically
            - Likewise you can repeat the same steps and let VM gain access to the blob by selecting the `data reader role`
        
    - **Disk encyption for Azure VMs using Key vault**
        - When a VM is created there is already a default level of encryption on the disk called `SSE- server side encryption`
        - You can create another layer of encryption using Azure key vault (called `ADE- Azure disk encryption`) - the process involves creating a new encryption key in the vault and then applying it onto the disk in the VM. You can specify whether you are doing it for the operating disk
            - Go to `Vm resource || disks > additional settings`
                - You can select between OS disks or both
                - Slect the key vault from a list
                - Then create a new key
                - Then select the new key
                - **Note that the key vault and the VM resource must be in the same region**
    
    - **Microsoft active directory**
        - You assign all devices to a domain and then link that domain to active directory
            -Authenticate/authorize users
            -Enforce security policies
            -Validate user permissions
        - Azure active directory and microsoft active directory are two different things
        - Microsoft active directory is what is installed onto a local on premisis server
        - **Azure AD connect** is then used to link the microsoft active directory with Azure AD connect
        - Some applications (legacy applications) may only run with microsoft active directory and not azure active directory
        - This connection allows users to make use of local microsoft active directory features as well as use the applications on Azure using the same user credentials
        - **Azure active directory domain services**
            - Search for Azure Domain service and create this resource
            - It creates a V net and VMs inside the Vnet - Takes about 45 minutes to complete
            - This will create a domain. You create the VM and then install microsoft active directory on it and then connect it to the domain

## Designing Data storage

- Types of data stores:
    - Storage accounts: 
    - SQL database: Install Oracle Microsoft SQL server, MYSQL database
    - Azure SQL database - platform as service
    - Azure SQL database Elastic pool - resources are shared across multiple SQL databases
    - Azure SQL managed instance - SQL database gets deployed onto a virtual network - Most secure, the communication between the database and the app does not happen over the public internet
    - Managed instance with CosmosDB - comprises of SQL api, table api, cassandra api, mongo api and gremlin api
    - Azure Cache for Redis - In memory datastorage
- **Authorization options**
    - `Azure active directory` - most secure
        - This is available for blob service (storing objects)
        - `Storage account || container` to see if you have blob data in place
        - Create a user in Azure active directory `Azure active dirctory > all users`
        - Then go to `storage account || access control > add` to assign a role and select the user you just created
            - Example roles to choose from: reader, storage blob data reader
        - Once you create a user in Azure AD, you will be able to directly log into Azure from the storage explorer
    - `Access keys `- easiest, but less secure - These are unique to a storage account - downside: they grant access to the entire storage account not just a particular service. You can access the keys at `storage account || access keys` - You can manage the storage accounts using **Azure storage explorer**. The keys link the storage explorer on your computer to the storage account to Azure. You can regenerate the keys is you suspect that an unauthorized access has been obtained
    - Creating shared `access signatures` - authorizing service, time bound access
        - Go to `storage account || shared access signature`
            - You have the option of setting the start and end time validity of signature - For example you can create a shared access signature to an external audit company
            - You can also define the allowed IP ranges
            - This generates a connection string
            - When you are linking to the Azure storage explorer, you can select Shared access signature instead of the keys

- **Storage Account access tiers**
    - Hot (frequent, default), Cool (atleast 30d storage) and Archive(atleast 180 storage) - configured by implementing lifecycle rules
        - `storage account || lifecycle management > add rule`
        - Rehydration: Accessing archived data (Archive -> hot or cool) - process takes time
    - Size based pricing: First 50TB is expensive then then next 450 and over 500 TB
    - Storage costs: size + read/write operations. Reading costs are very high for reading data in archived form + early deletion fee
    - You can select these settings: `storage account || configuration`
    - You can also change the tier for a particular data object e-g `storage account resource > images container > change tier`

- **Storage encryption**
    - `Storage account resource || encryption`
        - default option `microsoft-managed keys` alternative `customer-managed keys`
        - This default encryption is applied to data when not in use, when the data is in use by the app then it is decrypted
    - For custom control first **Create a key vault** - must be in the same location
        - `key vault || keys > generate`
    - Then when you are at `storage account || enctyption > customer-managed, select from key vault, select a key from vault and key`
        - This allows you to select the key vault and from the keys in it

- **Setting up access policies**
    - This can be applied at the container level
    - `container || policies` you can specify policies like read
    - When you create an access signature for access through the storage explorer, you can tie this policy to the signature. The storage explorer is bound by the policies tied to the access signature when you register a storage account using its URI
    - If you go back and change the policy linked to the storage container, that way you can also remove access, for example if you remove read privilidges
    - ANother option is **imutable blob storage**
        - `container in storage account || access policy > add policy under immutable blob storage`
        - Here you can select from further 2 types : 1) Time based retention -No one can change anything in the container for specified number of days
                                                   - 2) Legal hold

- **Azure SQL deployment**
    - Deploying SQL server onto a VM - This is the `infrastructure as service` option, benifit is pay as you go subscription option
    - The second option is deploying SQL server as `platform as service` Here you do not really configure any VMs - this is automatically taken care of by Azure - Benifits, automated backup, patching etc
    - The third option is `Azure SQL Manged instance` - useful if you are migrating, native Vnet integration
    - **Costing**
        - Based on DTUs - database transaction units- Basic, standard, premium (2ms vs 5ms latency, 25 vs 4 input output operations)
        - cost saving options: 
            - Using hybrid benifit using existing licenses
            - serverless - Auto-pause when not in use, scaling based on demand
            - You can scale up to 100TB storage
    - **Deploying an SQL server on Azure VM**
        - Search SQL server in main resources
            - Under `publisher`, choose `Microsoft`
            - You can select `SQL server 2019 on Windows server 2019` ( the latest version) - This will automatically create the VM and install SQL server init
            - Select the standard version from the drop down
            - Configure for VM and Vnet as usual, it will assign a public IP address
            - **SQL server settings tab** - the server that will be installed
                - Connectivity - private (within Vnet), open port 1433.
                - Enable SQL authentication
                - Then you can configure storage for the SQL server and select whether you aready have an SQL server license
    - **SQL database auditing**
        - Tracking database logs and rights
        - You can eneble it at the server or database level
        - Required for regulatory compliance
        - `database rsource || auditing > auditing on` Here you also have the option of doing so at server level
        - You can direct the logs to a storage account
        - You can also view the logs - the command query for SQL, user, IP
            - `databse resource || auditing > view audit logs at the top`
        - The storage account must be in the same region as the SQL server
    
    - **SQL diagnostics**
        - `database resource || diagnostic settings > add diagnostic settings`
            - gives you option to archive data to a storage account, send data to log analytics, select data to be sent
            - You can also specify the retention period for data of logs -Note that this retention setting does not affect the logs in the workspace. For changing policies in the retention in workspace, you have to go to the database workspace resource
        - For looking at usage and the estimated costs you can go to:
            - `database workspace resource || usage and estimated costs > data retention`
            - By default data retention is for 30 days - can be increased
    
    - **Data masking**
        - Credit card masking rule and email masking rule in Azure
        - `SQL database from resources || dynamic data masking under security`
            - This setting will automatically detect the variables from a database that can be masked and offers recommendations
            - You can also do this manually by clicking `add mask`, select the table, column - then you can click add and save
            - Masking only works when a non-privilidged user makes a query in the database
        - Note that for giving access to SQL database, you will have to first create a new user and then you will grant access to that newly created user
            - This can be done from the **Microsoft SQL server management studio**
    
    - **Transparent data encryption**
        - By default it is already enabled for a SQL database
            - `database resource || transparent data encryption under security`
            - Only encrypts data at rest
    - **Always encrypted feature**
            - You can also select specific tables that can be encrypted
            - Also encrypts data in transit
            - Two types: **Deterministic** - less secure but allows for point lookups, equality joins, grouping and indexing
                        - **Randomized** - More secure, but does notallow point lookups, equality joins, grouping and indexing
            - Once the encryption is enabled 2 keys are generated: 1) `column master key` and 2) `column encryption key` These are stored in a separate place such as the key vault or windows certificate store
            - The `user` who can use these keys must have the following permissions: `create, get, list, sign, verify, wrapkey and unwrapkey`
                - You can assign these user roles by going to the `vault resource || access policies`
                    - Then select the user and under the key permissions assign the privilidges under cryptogenic, then save
            - Then from the SQL server management studio - select the database and tables right click `encrypt columns` where you can select the column to be encrypted and then select the option of `deterministic` or `randomized` type
            - Next, you specify where to store the encrypted key: `Azure key vault or windows certificate` - This will require signing into the Azure account through the management studio. Then you can select the vault in the studio which is available from your subscription
            - Next from the management studio under security folder you can see the 'always encrypted keys' sub-folder which will show the key

    - **Creating Azure SQL managed instance**
        - Go to main resources and create `SQL managed instance`
        - It is helpful for SQL migration to Azure from on premesis environment
        - There are no DTUs with managed instance, there are basic and standard tiers
        - This creates a virtual network that is connected to the managed instance
        - You also have the option of geo-replication

    - **Query performance insights**
        - You can also find out details about queries and some basic informtion
        - Go to the `database main page || query performance insight under intelligent performance`
            - There you will see the query that is run and some basic statistics, if you click it you can actually see the query
        - **Automatic tuning**
            - Going to `databse main page || automatic tuning under intellegent performance` - This is to optomize performance
                - automatic option and manual option based on query performance insight tools

    
    - **Azure Redis Cache**
        - This is a platform managed service - in memory data store in SSDs
        - This service caches the frequently requested data for increasing speeds
        - The user makes a request for the first time, then the app fetches the data from SQL and stores inside the `in memory data store`
            - If the user makes the same request again, then it will fetch it from the in-memory store rather than executing a full scale query from the SQL
        - **Creating Redis cache**
            - Search for redis cache in main resources > create
            - From redis.io you can install the tool in cluster
            - Once created go to the `resourse > console` This opens the console and you can run commands from there
            - For `integration with an application`, you will have to code such that first the app screens the cache to see if an id already exist, if not then make the query into the database. you will also have to link redis with the access key

    - **Azure cosmosDB**
        - Create Azure cosmos DB account from the main resources
        - This offers noSQL options including core, Azure cosmos for MongoDB API, Azure table, gremlin etc
            - Select capacity as `provisioned throughput` - charged based on request unit - you get 400RU/s and 5GB storage for free in an account
            - `CosmosDB account || replicate data globally`
            - You can add or remove regions or enable multi-region writes - Selection of these regions replicated tha data onto those regions
            - **CosmosDB resource tokens**
                - Consists of user account master keys
                - For secure access the master keys are stored in the middle tier
                - The the user is first authenticated by the app, the user is again verified in the middle tier, if verified it then generates a resource token from cosmosDB account for the app, Once the app gets the resource token, it can access the CosmosDB account

    - **Azure Data lake storage**
        - Additional feature on top of Azure blob storage for enterprize level data storage - processing vast amounts of data (in petabytes)- especially useful for big data analytics. Offers file level security and scale
        - Creats hiearchy of directories for efficient data access
        - Helpful for storing large amount of data in raw format including documents etc - it has not been processed
        - This is NOT the same as SQL datawarehouse - the key difference is that here the data is already processed and in structured format
        - You can activate the data lake storage feature while creating a storage account
            - under advanced check `enable data lake storage gen2`
        - Once the resource is created, if you go to the storage account and create a container, in the container, you will now have the ability to `add dirctory`

    - **Azure databricks**
        - Provides unified anaytics platform that is built on Apache Spark
        - Apache spark engine will need to be installed onto the machines along with the libraries - also referred to as provisioning
        - `Databricks` basically takes care of setting up spark engine which includes
            - Computing the infrastructure
            - Takes care of underlying storage
            - abstraction of data layer
            - Adds capabilities of machine learning
            - Create visualizations
            - Allows integration with other Azure resources: V nets, Azure Security, role based access control
            - **Components of Azure databricks workspace**
                - Clusters - used to run data processing jobs
                - Notebooks - interacting with data and creating visualizations
                - Libraries - to be run on cluster
                - Data - To be mounted onto the databricks workspace
            - **Creating Azure databricks workspace**
                - Search from resources and then create it
                - Pricing tier: standard (Apache spark, Azure AD), premium, trial
                - Databricks is a platform as service managed resource
                - Under **networking tab**
                    - no for deploying into Vnet - this will create databricks clusters (it will automatically create a new Vnet when created), yes if you already have a Vnet where you want to deploy databricks
                    - Once the workspace is created, go to the resource 
                    - Notice that in the home page you see that a `managed resource group` is already created
                        - This automatically creates storage account, NSG and a workers Vnet
                    - If you scroll down, you can see that there is a launch button to open the workspace - it will be linked to Azue AD
                        - In the workspace you have the options to create notebooks, table, cluster etc
                        - **Creating a cluster**
                            - Click create cluster - it is the cluster of machines
                            - cluster mode: high concurrency, standard, single node
                            - select the runtime version. ML denotes versions with the option of machine learning
                            - You can enable autoscaling
                            - Specify the underlying workers - machines responsible for data processing (min and max)
                            - If you checkmark `enable autoscaling` then you it will automatically spin new clusters as needed
                            - You can also specify when you can terminate cluster after specified minutes of inactivity
                            - You can also terminate and restart the cluster
                        - **Creating a notebook**
                            - click create new notebook
                            - select underlying language for notebook such as python of SQL - attach it to the appcluster that you created earlier > create
                            - A notebook executes code - you can type in commands
                            - Commands can be run in the form of cells
                        - **Reading data in CSV file using notebook**
                            - Add a container in the datastorage account and upload a csv file
                            - If the file size is greater than 2mb, you cannot visualize the data
                            - Using python, you can first create a mount point
                            - for authentication/authorization you can use account keys or access signatures
                                - You can get the keys from `storage account || access keys`
                                - In the notebook, you will first have to establish authentication - this is done with python code that incorporates the account key into the command to link with the storage account where the csv file is present - this step is called `creating a mount point`
                                - Next you can write the code to list the files in the storage
                                - Next you can write code to read csv file using spark in a new cell- this consists of two steps-first you create a data frame and then display the data

    - **Azure synapse analytics**
        - It is compiling together data from all sources into a single location called SQL database warehouse
        - Create an SQL database
            - Under additional settings you have the option of seleting "sample" in `use exisiting data ` configuration
            - Once the resource is created, from its hompage you can get the server name (the website)
        - Connect with the database engine in Azure using `SQL server management studio`
            - here you will see that under database that the sample tables are already created
        - **Creating Azure synapse analytics**
            - The first step is to create an Azure synapse analytics workspace
            - You can searh for it in general resoures and then create
                - Select the region, subscription as usual
                - You will need to create an **Azure datalake account** or choose an existing one
                - You will also have to create a **file system name**
                - With this what you create is a synapse workspace - so when you go to the resource homepage, you will see the homepage of the **workspace** of **Azure Synapse**
                - Points to note on the homepage of the Azure analytics workspace
                    - SQL admin username
                    - Dediated SQL endpoint - this allows you to connect with a dedicated SQL pool which is a dedicated 
                    - Serverless or service SQL endpoint - You create a dedicated SQL pool inside an Azure analytics workspace
                    - Undrer `Azure analytics workspace resource || Analytics pool` you an see the built in SQL pool - This is the default pool that is created when you create an Azure synapse workspace. You can create additional pools by `azure analytics workspace resource
        - **Creating a dedicated SQL warehouse or pool**
            - `Azure analytics workspace resource > New dedicated SQL pool` This will create a new pool
            - This is required if you want to perform analytics on data
            - Once the pool is created, you can go to the resource homepage
            - On the homepage you will see a weblink under **dediated SQL endpoint** - this is like the server address to the SQL pool, you can connect to it by pasting this link in the server when using the **SQL server management studio**
            - **Azure Data Factory**
                - Allows Extraction, transformation and loading of data
                - Components:
                    - **Linked service** the function that ingests the data
                    - Datasets - it is the **linked service object** - which is the datastructure inside the data store
                - Process:
                    - `Connect` with data sources
                    - `Ingest` the data from sources
                    - `Transform `the data in pipeline
                    - `Publish` the data in forms of Warehouse, SQL database or Cosmos DB
                - A **pipeline** is nothing but a logical grouping of activities inside the data factory
                - Note that SQL server and database are 2 different things
## Business Continuity

- **RPO** - **Recovery point objective** - how much data can you afford to loose after the recovery is made - maybe 5min from the time of shoud down or 2hrs that you may be ok to loose
- **RTO** - **Recovery time objective** - How long will it take to get the database up and running -for apps that are highly critial , it must be back up and running withn 10 minutes

    - **options**
        - `Backup and restore` - High chance of data loss
        - `Continues replication`- maybe every 5 minutes - you are relicating the changes
    - With **Azure SQL database backup** full backups are taken every week differential backups are taken every 12-14hrs and transactional log backups are taken every 5-10 minutes
    - **Restore options**
        - `Point in time` restore - restore of database is done on the same server - can be done for both existing and deleted databases
        - `Geo-restore` - restore on a different location
        - Backups can be stored for up to 10 years in storage accounts
    - **Point in time restore**
        - Go to `database homepage > restore` This is done based on an earlier restore point - these backups were automated backups
        - Once the backup restore is complete, you will see a new database in the SQL databases in restore which is the copy of the database that you backed up. Even if you do not have any data in the database, it can still take up to 10 minutes for the backup to complete the restore. The copy is created on the same database server
        - You can delete a database by going to the `homepage of that database > delete`
        - You can locate the deleted database by going to `database server > deleted databases`
            - Once you click the deleted database that you see here, there is an option to restore based on a restore point
- **Zone redundancy**
    - Replicates the database to mulitple zones which means that the data is replicated to nodes in different data centers and is replicated synchronously
    - You will have to pay for data that is replicated in every availability zone - typically x3
    - The data is initially stored in the primary availability zone, then to the other 2 from the primary. This affects the latency
    - Options:
        - General purpose
        - Hyperscale
- **Geo-replication**
    - Data is coninuesly replicated into a readable secondary database in the same or a different data center (same server or a different server)
    - This feature is not supported by Azure SQL managed instance
    - The secondary database is read-only no writes
    - One benifit of the primary and secondary system of databases is that you an then focus the application related database tasks with the primary and you can focus the secondary for reports and failover (when the primary fails). A failover can result in a small dataloss
    - Auto-failover - you can automate the process of replication and failover - can be done automatically via a policy or manually 
    - `database resource || geo-replication under database management`
        - Here you will choose the location where you want your data to be replicated to
        - You will probably have to **create a new database server** for the new location - Allow az services to access the server. Note that the firewall rules do not get copied from the primary database server to the secondary database servers and you will not be able to connect to the geo-replication database via SQL server manager if the ip policies are not configured in the geo-server db
        - The name of the geo db is the same as the db which is a copy of, but it has a marker geo and it is a read only copy
        - You can configure the IP settings by going to the `replicated db > webserver address > show firewall settings > Add client ip address`
    - Geo-replication failovers are always done manual

- Geo-restore and geo-replicated backups are slow - higher RTO and RPO 12/1hr
- Auto-failover and manual database failover has much lower RTO RPO as less as 5 s or RPO
- You can test failover by first creating a geo replica of a database, then making entries into the primary and then executing the failover from the secondary-geo db that you previously created. 
- **Manual failover**
- `SQL databases > primary db > primary db server || geo-replication`
    - Here where you see the secondary db in the different region, the one which is readable, right click and select `forced failover`
    - Also remember to add an IP address policy in the firewall settings of the secondary database if you want to connect to it via the SQL database server manager
    - This will update the current version of the secondary database with the changes from the latest version of the primary database
- **Automated failover**
    - `SQL databases > primary database > primary database server || geo-replication`
    - Stop the replication by the secondary database and then delete the secondary database (not the server) from the SQL databases list
    - Then from the primary database through the SQL database manager - create a new table in the parimary database
    - Then go to the server page for the `primary SQL database || failover groups > add group`
        - choose the replicate server
        - read/write policy as is
        - Once the group is created and you go to its 
        - One change is that instead of using the server weblink, now you are using the read-write link from the failover group
        - The automatic failover will kick in after a certain number of data entries and time - you do not have to manually trigger it
- **Storage account redundancy**
    - There are multiple copies
    - Types:
        - `Locally redundant` - 3 copies 
        - `Zone redundant` - protects against data center level failures - the data is replicated at 3 availability zones that are within a single primary region
        - `Geo zone-redundant` - multi-region, can survive region level failure. There is a primary region and a secondary region. This is called paired region. The secondary region is automatically decided by Azure. Here the data is copied x3 in the primary as well as the secondary region using locally redundant storage, each. Only when the primary region goes down, does the secondary becomes available
        - `Read-access geo-redundant storage` - Both regions are available, regardless of the region where the failure occurs
        - **payment**
            - You pay for data in each region, both primary and secondary
            - You pay for data transfer b/w region
            - you pay for data transfer b/w availability zones

- **SLA - service level agreements for a VM**
    - If single instance, gaurantee of connectivity 95% this is with HDD disk - cheapest
    - Next up is VM with SSD - 99.5% connectivity
    - Bext up is VM with SSD premium or ultra HD, connectivity of at least 99.9%
    - If there are 2 VMs, both in the same availability set connectivity to at least 1 is 99.95%
    - If thre are 2 VMs, in 2 or more availability zones, connectivity to at least 1 is 99.9%

- **Backup for Azure VMs**
    - `VM resource || backup` The backup is stored in a backup vault, the backup service will automatically create a new vault if one does not exist already
    - You can schedule a backup on a daily basis or weekly basis - you cannot have constant backup with virtual machines - not more than once per day
    - Before the backup is sent to the vault- it is stored in the VM for 2 days - You can define all these rules in a policy. This VM backup allows instant restore
    - The backup itself in the vault is retained for 30days
    - If you want constant backup with VMs you can use `Vm resource || disaster recovery` Constantly replicates data onto a secondary location

- **Azure site Recovery Service**
    - Goals: business continuity and recovery in the setting of an outage
    - Typically a company will have 2 data centers - one is primary and another is the secondary or backup - both on premesis
    - There is also an option of switching the application onto the servers in Azure instead of the primary data center
    - The RPO in this setting is low - as the replication frequency is high thus the data loss is only for the recent 30s for Hyper-V
    - The RTO is alo low. As the data is replicated in a secondary VM, it can be restored quickly
    - **Disaster recovery for an Azure Virtual Machine**
        - This is a feature available with a VM that can be activated to backup the whole machine into a recovery vault and then using the backup to recover and recreate it
        - When you enable disaster a duplicate virtual network and a storage account are created
        - Changes to the VM are first weitten to a cache account which is then replicated into the target storage
        - The VM is first stored into a cache storage account which creates a restore point in the data center of a destination region (usually differnt from the primary region) from which a new virtual machine (target VM) which is the replica of the VM from the primary region is spun
        - The storage accounts are uaually automatially created in the destination region
        - If you have a low RPO and RTO - the data is continurely replicated into the destination region
        - You can review the `Vm resource page || disaster recovery under operations ` to see the current settings for data recovery. There you can also see the RPO and the **latest recovery points** - these will be displayed if a recovery vault is already configured
            - Othersise This is also where you can enable and configure the process of disaster recovery if one does not exist already
            - If you go under `advanced settings` you can set various settings for cache account, replication policy etc
            - You can go under `start replication` to initiate the process of backup and recovery
            - It will also create recovery services vault automatically if one does not exist

        - Once you have created a replica, It will also create a new resourse group, you can get to it by locating it under the resources. And when you get to the home page of this resource group - you will be able to see everything that is in it including the new replica that is created
            - What the recovery vault creates is a Vnet and the replia copy - No VM will be present - the later is only present when you **initiate a failover** You can get to the recovery services vault from the main resource page
                - You can initiate a **test failover** `Vm resource page || disaster recovery under operations > test failover `
                - You can select between app consistent (higher RPO) or latest processed options when creating a failover
                - `Vm resource page || disaster recovery under operations >failover ` to run actual failover
                - `recovery vault resource || replicated items under protected items` will show you whether your current VM has been nicely and continuesly being backed up using the primary VM
                - `recovery vault resource || site recovery inrastructure || replication policies` Her you can see the default policy or modify it
            - You can also specify the replication policies where you can define things like retention period
            - Once you have run the failover, you can go to the VM from the main resources and you will be able to see 
            - Finally you can also cleanup the test failover
        - **Azure site Recovery - Hyper V**
            - Hyper V -It can be on a local premisis, you may be looking to replicate the hyper V from local to Azure
            - Here, you are not copying hyper V to hyper V, but actually copying VM from a local hyper V to Azure hyper V
            - You will have to install an agent on both HyperV s (local+Azure)
            - Agent will have to be installed on every VM that is part of the hyper V (often each hyper V contain cluster of VMs)
            - Go to `reovery services vault site recovery || site recovery` to prepare infrastructure
                - Here you can specify the source and destination, select hyper V under are machines virtualized. System center is typically when you have multiple hyper V hosts, so here you can select no. 
                - The next step is to prepare source > `+hyper V site`, `+hyper V server`- This is to ensure that 
                - The hyper V host needs to be registered into the recovery services vault
                    - Install the agent on the hyper V host (Azure site recovery provider - this is the hyper V of company on local premesis). You can do this by downloading the file on your laptop and then mapping your laptop drive into the hyper V and then copying the file into the hyper V and then running it from hyper V
                - Before you `transfer` the VM, you will have to create a **storage account** and **Vnet** and **subnet** on the target on Azure
                - You can then specify a policy for replication, defining: copy frequency, recovery point retention, app-consistent snap shot etc
                - The VMs on the local hyper V will be automatically detected
                - Under properties you complete the mapping - define the target VM name
                - Finally assign the replication policy that you have previously created and then you can enable the replication
                - Once these steps are complete you have completed the prepare infrastructure part
                - You can then monitor the replication by going to the `recovery services vault resource || replicated items`
                - Likewise you can also run the test failover like with recovery services
        
        - **Azure Key Vault High availability**
            - This is to ensure that if one region goes down. You can have the key vault available in another region in read-only format
        - **VM Scale Sets**
            - Scaling VNs based on demand -scale up or down 
            - You can create a VM scale set by searching VM scale set in the resources
            - You can create an **Azure Service bus resource**
            - The scale set and the service bus are needed for scaling, expecially if you want to scale based on messaging
            - You can go to reseources and find `virtual machine scale set` then you can create the resource using the standard settings
                - **scaling**
                    - you can leave manual for the scaling option and specify 1 as the initial instance count
            - You can scale the resources based on **Azure Service Bus que**
                - Find the resource `service bus` from main resources. Once you hit create - it will take you to the `create namespace` page
                    - In the namespace resource you can create a queue by going to its homepage and clicking `que`, next you cn go to scaling             
                - Once the `resource is created go to it || scaling`
                    - Under instance limits select minimum as 1 and maximum as 3
                    - `resource name || add rule`
                        - You can define rules based on CPU percent usage or you can select service buse que, which goes by the number of messages
                        - Initially you can scale based on the percent CPU usage. Alternatives include scaling based on storage que or service bus que
                        - If you want to scale based on the que, then you will be able to see the service bus que that you created earlier. There you can specify the number of messages processed
            - **Availability set** Service level agreement of 99.95%
            - Consists of update and fault domains
            - Another important point linked to the upate and fault domain is that of the **Fault domain** and the **update domain**
                - The fault domain is a separate power source - you can have your VMs spread out over 3 different fault domains - so if one fault domain does not work the others will be working and not all VMs will go down at once
                - The update domains can be up to 20 - there are the different VMs within the same fault domain that allows one VM to update without disruption. The updates are performed per server. So multiple VMs even if on different domain will not be availabe if they are on the same server which would mean that theya are also on the same update domain
                - The availability sets can only be specified at the time of creation of a VM. They cannot be modified once they are assigned
            
            - **Availability Zones** Service level agreement of 99.99%
                - Three different availability zones in a region -These are unique locations within a region
                - These protect against datacenter level failure
                - Each zone is a collection of one or multiple datacenters
                - More cost due to the communication b/w VMs across availability zones
            - You can select between availability zone vs availability set when creating a virtual machine - this is under the availability options
    
    # Azure infrastructure 

    - You can use Azure VM or Azure Web app feature
    - VM - you have total control, you can install any application, you are responsible for maintainance with updates, you will also have to manually implement scaling (via Azure VM scale sets) - This is infrasctructure as serive
    - Azure web app, you cannot install software. It is managed by Azure, you can scale the app - This is platform as service
    - **Azure function** is serverless service - you are billed for the duration of execution - cheeaper than VM
    - Azure functions are not meant to run the web applications - for the latter you can select a VM or Azure webap

    ## Azure functions
    - For example, one fuction can be making database calls
    - Publishing an Azure funtion onto an Azure function app
        - Search for **Function app** in Azure and create this resource - select the runtime stack
        - Then you will have to publish your function using the virtual studio
    
    ## Virtual machines
    - Various sizes and classes
        - **General purpose**
            - AV2 - Entry level dvlopement and test environment
            - DCsV2 - More secure, latest processor, can protect data while in use
        - **Compute optimized**
            - Higher CPU to memory ratio - good for medium traffic and application servers
        - **Memory Optimized**
            - EV,ESV3 series Higher memory to CPU ratio - good for hosting databases
        - **Storage optimized**
            - Big data applications, SQL and noSQL databases
    
    ## Azure Batch Service
    - large scale paralell high performing jobs
    - You create Azure **Batch Account** and Azure **Storage Account**
    - The batch account basically contains a pool of VMs that will pull the videos from the storage Account
    - It consists of **Nodes** to run the applications - each node is a VM - The nodes have the capabilities of running certain scripts
        - Dedicated nodes - to make sure that a job is done - more expensive
        - Low priority nodes
    - Search for batch service resource from general resources
        - Assign a new batch account name
        - Select storage account
        - Once created - go to the resource
            - Specify application
            - FFmpeg tool - converting video and audio file using Azure batch service. Will need to convert to zip file if .exe before it can be uploaded it the storage account
            - You an add pool of VMs
            - While you are creating the pool you will create a V net as well as a subnet
            - Create a video container in the storage account and then upload a video file into this container
            - Going back to the batch account and then assigining a job
            - Then add and run a task on the VM. You can type in a command while creating the task. Select the resource file which includes selecting the storage container containing the video
            - Then you also select the application package



